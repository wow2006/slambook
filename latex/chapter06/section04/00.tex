\section{Summery}

This section introduces a nonlinear optimization problem often encountered in SLAM: the least squares problem consisting of the sum of squares of many error terms. We introduced its definition and solution, and discussed two main ways of gradient descent: the Gauss-Newton method and the Levenberg-Machalt method. In the practical part, the handwritten Gauss-Newton method, Ceres and g2o optimization libraries are used to solve the same curve fitting problem, and they are found to give similar results.

Since we haven't talked about the Bundle Adjustment in detail, we chose a simple but representative example of curve fitting in the practice section to demonstrate the general nonlinear least squares solution. In particular, if you use g2o to fit a curve, you must first convert the problem to a graph optimization, defining new vertices and edges. There are some roundabouts in this approachâ€”the main purpose of g2o is not here. In contrast, it is natural for Ceres to define the error term for the curve fitting problem, because it is an optimization library itself. However, more problems in SLAM are how to solve an optimization problem with many camera poses and many spatial points. In particular, when the camera pose is expressed in Lie algebra, how the error term is calculated with respect to the derivative of the camera pose will be a matter worthy of detailed discussion. We will find in subsequent content that g2o provides a large number of ready-made vertices and edges, which is very convenient for camera pose estimation. In Ceres, we have to implement each Cost Function ourselves, which is inconvenient.

In the two programs in the practice section, we did not calculate the derivative of the curve model for the three parameters, but used the numerical derivation of the optimization library, which makes the theory and code simple. The Ceres library provides automatic derivation and runtime numerical derivation based on template elements, while g2o only provides a way to derive runtime values. However, for most problems, if you can derive the analytical form of the Jacobian matrix and tell the optimization library, you can avoid many problems in numerical derivation.

Finally, I hope that readers will be able to adapt to the extensive use of template programming by Ceres and g2o. Maybe it will look scary at first (especially Ceres sets the parentheses of the residual block and the code for the g2o initialization part), but after familiarity, it feels like this is natural and easy to extend. We will continue to discuss sparsity, kernel functions, Pose Graph and other issues in the SLAM backend.