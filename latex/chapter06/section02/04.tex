\subsection{Summery}

Since I don't want this book to become a mathematics textbook that is a headache, here are just two of the most common nonlinear optimization schemes, the Gauss-Newton method and the Levinberg-Marquart method. We have avoided many discussions on the nature of mathematics. If you are interested in optimization, you can read a book that specializes in numerical optimization (this is a big topic)\cite{Nocedal2006}. The optimization methods represented by the Gauss Newton method and the Levenberg-Marquart method have been implemented and provided to users in many open source optimization libraries. We will conduct experiments below. Optimization is a basic mathematical tool for dealing with many practical problems. It plays a central role not only in visual SLAM, but also in other fields like deep learning. It is also one of the core methods for solving problems. The order method is mainly). We hope that readers will be able to understand more optimization algorithms based on their capabilities.

Perhaps you have discovered that both the Gauss Newton method and the Levinburg-Marquardt method need to provide the initial value of the variable when doing the optimization calculation. You may ask, can this initial value be set at will? of course not. In fact, all iterative solution solutions for nonlinear optimization require the user to provide a good initial value. Since the objective function is too complex, the change in the solution space is difficult to predict, and providing different initial values for the problem often leads to different calculation results. This situation is a common problem of nonlinear optimization: most algorithms are prone to fall into local minima. Therefore, no matter what kind of scientific problem, we should provide scientific basis for the initial value. For example, in the visual SLAM problem, we will use the algorithms such as ICP and PnP to provide optimized initial values. In short, a good initial value is very important for the optimization problem!

Perhaps the reader will also have questions about the optimization mentioned above: How to solve linear incremental equations? We only talked about the incremental equation is a linear equation, but directly inverting the coefficient matrix is not a lot of calculations? of course not. In the visual SLAM algorithm, the dimension of $\Delta \bm{x}$ is often as large as several hundred or thousands. If you are doing large-scale visual 3D reconstruction, you will often find that this dimension can be easily reached. Hundreds of thousands or even higher. Inverting such a large matrix is unaffordable for most processors, so there are many numerical solutions for linear equations. There are different solutions in different fields, but there is almost no way to directly find the inverse of the coefficient matrix. We will use matrix decomposition to solve linear equations, such as QR, Cholesky and other decomposition methods. These methods are usually found in textbooks such as matrix theory, and we will not introduce them.

Fortunately, this matrix in the visual SLAM tends to have a specific sparse form, which provides the possibility to solve the optimization problem in real time. We will explain its principles in detail in Lecture 9. Using the sparse form of the elimination, decomposition, and finally the solution increment, will greatly improve the efficiency of the solution. In many open source optimization libraries, variables with more than 10,000 dimensions can be solved in a few seconds or less on a typical PC, and the reason is to use more advanced mathematical tools. The visual SLAM algorithm can now be implemented in real time, and thanks to the fact that the coefficient matrix is sparse. If the matrix is dense, I am afraid that optimizing such a visual SLAM algorithm will not be widely adopted by the academic community \textsuperscript{\cite{Lourakis2009, Sibley2009a, Triggs2000 }}.
