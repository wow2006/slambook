\section*{ Exercises}

\begin{enumerate}
	\item proves that the linear equation $\bm{A} \bm{x} = \bm{b}$ when the coefficient matrix $\bm{A}$ is overtimed, and the least squares solution is $\bm{x} = (\ Bm{A}^\mathrm{T}\bm{A})^{-1}\bm{A}^\mathrm{T} \bm{b}$.
	\item investigates the advantages and disadvantages of the steepest descent method, the Newton method, the Gauss-Newton method, and the Levenberg-Marquart method. In addition to our Ceres library and g2o library, what other optimization libraries are there? (You may find some libraries on MATLAB.)
	\item Why is the Gauss-Newton method's incremental equation coefficient matrix not correct? What is the geometric meaning of uncertainty? Why is the solution unstable in this case?
	\item What is DogLeg? What are the similarities and differences between it and the Gauss Newton method and Levinburg-Marquart method? Please search for the relevant material \footnote{\mbox{for example,}\url{http://www.numerical.rl.ac.uk/people/nimg/course/lectures/raphael/lectures/lec7slides.pdf}. }.
	\item Read Ceres' teaching materials (\url{http://ceres-solver.org/tutorial.html}) to better understand its usage.
	\item Read the documentation that comes with g2o. Can you read it? If you still can't fully understand it, please come back after the 10th and 11th lectures.
	\item(optional) Please change the curve model in the curve fitting experiment and optimize the experiment with Ceres and g2o. For example, more parameters and more complex models can be used.
\end{enumerate}