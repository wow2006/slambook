\subsubsection{Install Ceres}

In order to use Ceres, we need to compile and install it. Ceres' github address is: \url{https://github.com/ceres-solver/ceres-solver}, but you can also use the Ceres in the 3rdparty directory of this book code directly, so that you will use exactly the same as me. version.

Like the library I encountered before, Ceres is a cmake project. First install its dependencies, you can use apt-get to install in Ubuntu, mainly some of the logs and test tools used by Google itself:

\begin{lstlisting}[language=sh,caption=terminal input:]
Sudo apt-get install liblapack-dev libsuitesparse-dev libcxsparse3 libgflags-dev libgoogle-glog-dev libgtest-dev 
\end{lstlisting}

Then, go to the Ceres library directory, compile and install it using cmake. We have done this process many times, and we won't go into details here. After the installation is complete, find the Ceres header file under /usr/local/include/ceres and find the library file named libceres.a under /usr/local/lib/. With these files, you can use Ceres for optimization calculations.

\subsubsection{Use Ceres to fit curves}
The following code demonstrates how to solve the same problem using Ceres.

\begin{lstlisting}[language=c++,caption=slambook/ch6/ceresCurveFitting.cpp]
#include <iostream>
#include <opencv2/core/core.hpp>
#include <ceres/ceres.h>
#include <chrono>

Using namespace std;

// The calculation model of the cost function
Struct CURVE_FITTING_COST {
CURVE_FITTING_COST(double x, double y) : _x(x), _y(y) {}

// Calculation of residuals
Template<typename T>
Bool operator()(
Const T *const abc, // model parameter, 3D
T *residual) const {
// y-exp(ax^2+bx+c)
Residual[0] = T(_y) - ceres::exp(abc[0] * T(_x) * T(_x) + abc[1] * T(_x) + abc[2]);
Return true;
}

Const double _x, _y; // x,y data
};

Int main(int argc, char **argv) {
Double ar = 1.0, br = 2.0, cr = 1.0; // true parameter value
Double ae = 2.0, be = -1.0, ce = 5.0; // estimate parameter value
Int N = 100; // data points
Double w_sigma = 1.0; // noise sigma value
Double inv_sigma = 1.0 / w_sigma;
Cv::RNG rng; // OpenCV random number generator

Vector<double> x_data, y_data; // data
For (int i = 0; i < N; i++) {
Double x = i / 100.0;
X_data.push_back(x);
Y_data.push_back(exp(ar * x * x + br * x + cr) + rng.gaussian(w_sigma * w_sigma));
}

Double abc[3] = {ae, be, ce};

// Build the least squares problem
Ceres::Problem problem;
For (int i = 0; i < N; i++) {
problem.AddResidualBlock( // Add an error term to the question
// Use automatic derivation, template parameters: error type, output dimension, input dimension, dimension must be consistent with the previous struct
New ceres::AutoDiffCostFunction<CURVE_FITTING_COST, 1, 3>(
New CURVE_FITTING_COST(x_data[i], y_data[i])
),
Nullpt, // kernel function, not used here, empty
Abc // parameter to be estimated
);
}

// Configure the solver
Ceres::Solver::Options options; // There are a lot of configuration items to fill in
Options.linear_solver_type = ceres::DENSE_NORMAL_CHOLESKY; // How to solve the incremental equation
Options.minimizer_progress_to_stdout = true; // output to cout

Ceres::Solver::Summary summary; // Optimization information
Chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
Ceres::Solve(options, &problem, &summary); // Start optimizing
Chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
Chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
Cout << "solve time cost = " << time_used.count() << " seconds. " << endl;

// output result
Cout << summary.BriefReport() << endl;
Cout << "estimated a,b,c = ";
For (auto a:abc) cout << a << " ";
Cout << endl;

Return 0;
}
\end{lstlisting}

The places in the program that need to be explained are commented. As you can see, we used OpenCV's noise generator to generate 100 Gaussian noise data, which was then fitted using Ceres. The Ceres usage demonstrated here is as follows:

\begin{enumerate}
	\item Defines the class of the residual block. The method is to write a class (or structure) and define the () operator with the template parameter in the class, so that the class becomes a \textbf{fiction} (Functor)\footnote{C++ terminology The class of the bracket operator is like a function when using the bracket operator. }. This way of defining makes Ceres call the a<double>() method on an object of that class (such as a) just like calling a function. In fact, Ceres will pass the Jacobian matrix as a type parameter to this function, thus implementing the automatic derivation function.
	\item The double abc[3] in the  program is the parameter block, and for the residual block, we construct the CURVE\_FITTING\_COST object for each data, and then call AddResidualBlock to add the error term to the target function. Since optimization requires gradients, we have several choices: (1) using Ceres' Auto Derivation (Auto Diff); (2) using Numeric Diff\footnote{automatic derivation is also done with numerical derivatives, but Because it is a template operation, it runs faster. }; (3) Deriving the analytical derivative form by itself and providing it to Ceres. Because automatic derivation is the most convenient in coding, we use automatic derivation.
	\item Autoderivation requires specifying the dimensions of the error term and the optimization variable. The error here is scalar, the dimension is 1; the optimization is three quantities of $a, b, c$, and the dimension is 3. Therefore, the variable dimension is set to 1, 3 in the template parameter of the automatic derivation class AutoDiffCostFunction.
	\item After setting the problem, call the Solve function to solve. You can configure (very detailed) optimization options in options. For example, you can choose to use Line Search or Trust Region, number of iterations, step size, and more. Readers can look at the definition of Options to see which optimization methods are available, and of course the default configuration is already available for a wide range of issues.
\end{enumerate}
	
Finally, let's take a look at the experimental results. Call build/ceresCurveFitting to see the optimization results:
\begin{lstlisting}
Iter cost cost_change |gradient| |step| tr_ratio tr_radius ls_iter iter_time total_time
0 1.597873e+06 0.00e+00 3.52e+06 0.00e+00 0.00e+00 1.00e+04 0 2.10e-05 7.92e-05
1 1.884440e+05 1.41e+06 4.86e+05 9.88e-01 8.82e-01 1.81e+04 1 5.60e-05 1.05e-03
2 1.784821e+04 1.71e+05 6.78e+04 9.89e-01 9.06e-01 3.87e+04 1 2.00e-05 1.09e-03
3 1.099631e+03 1.67e+04 8.58e+03 1.10e+00 9.41e-01 1.16e+05 1 6.70e-05 1.16e-03
4 8.784938e+01 1.01e+03 6.53e+02 1.51e+00 9.67e-01 3.48e+05 1 1.88e-05 1.19e-03
5 5.141230e+01 3.64e+01 2.72e+01 1.13e+00 9.90e-01 1.05e+06 1 1.81e-05 1.22e-03
6 5.096862e+01 4.44e-01 4.27e-01 1.89e-01 9.98e-01 3.14e+06 1 1.79e-05 1.25e-03
7 5.096851e+01 1.10e-04 9.53e-04 2.84e-03 9.99e-01 9.41e+06 1 1.81e-05 1.28e-03
Solve time cost = 0.00130755 seconds. 
Ceres Solver Report: Iterations: 8, Initial cost: 1.597873e+06, Final cost: 5.096851e+01, Termination: CONVERGENCE
Estimated a,b,c = 0.890908 2.1719 0.943628 
\end{lstlisting}

The final optimization value is basically the same as the experimental results in our previous section, but Ceres is relatively slower in speed. Ceres used about 1.3 milliseconds on my machine, which is about six times slower than the handwritten Gauss Newton method.

I hope that readers can get a general idea of how to use Ceres through this simple example. It has the advantage of providing an automatic derivation tool that eliminates the need to calculate a troublesome Jacobian matrix. Ceres' automatic derivation is implemented by template elements, which can be done automatically at compile time, but still be a numerical derivative. Most of the time, the book will still introduce the calculation of the Jacobian matrix, because it is more helpful to understand the problem, and there are fewer problems in the optimization. In addition, Ceres' optimization process configuration is also very rich, making it suitable for a wide range of least squares optimization problems, including various issues outside of SLAM.
